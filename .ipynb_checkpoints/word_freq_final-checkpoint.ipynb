{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import string\n",
    "import gensim\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.manifold import TSNE\n",
    "from adjustText import adjust_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('models/model_ep30.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t = time()\n",
    "\n",
    "words = []\n",
    "sentences = []\n",
    "chars = []\n",
    "word_vectors = []\n",
    "\n",
    "with open(\"data/data_origin.DUMP\", encoding=\"utf8\") as tsv:\n",
    "#     counter = 0;\n",
    "    for line in csv.reader(tsv, dialect=\"excel-tab\"):\n",
    "        sentence = line[2]\n",
    "#         counter += 1\n",
    "\n",
    "        # trim punctuation, make it lowercase\n",
    "        sentence = sentence.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "\n",
    "        all_words = nltk.word_tokenize(sentence)\n",
    "\n",
    "        for word in all_words:\n",
    "            if word not in stopwords.words('turkish'):\n",
    "                \n",
    "                words.append(word)\n",
    "                \n",
    "                if word in w2v_model.wv.vocab:\n",
    "                    word_vectors.append(w2v_model.wv[word])\n",
    "                \n",
    "                for char in word:\n",
    "                    chars.append(char)\n",
    "                \n",
    "#         if counter == 100:\n",
    "#             break\n",
    "            \n",
    "print('Time to tokenize: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate entropy\n",
    "entropy = 0\n",
    "decset = set(chars)\n",
    "freqdic = {} # holds the frequency of each char \n",
    "for c in decset:\n",
    "    freqdic[c] = dec.count(c)\n",
    "for c in decset:\n",
    "        prob = freqdic[c] / (1.0*len(chars))\n",
    "        information_content = np.log2(1.0/prob)\n",
    "        entropy += prob * information_content\n",
    "\n",
    "print('Std: ' + str(np.std(word_vectors)))\n",
    "print('Max: ' + str(np.max(word_vectors)))\n",
    "print('Min: ' + str(np.min(word_vectors)))\n",
    "print('Avg: ' + str(np.average(word_vectors)))\n",
    "print('Entropy: ' + str(entropy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = [w2v_model.wv[word] for word in words if word in w2v_model.wv.vocab]\n",
    "# Zip the words together with their vector representations\n",
    "word_vec_zip = zip(words, word_vectors)\n",
    "\n",
    "# Cast to a dict so we can turn it into a DataFrame\n",
    "word_vec_dict = dict(word_vec_zip)\n",
    "df = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Initialize t-SNE\n",
    "tsne = TSNE(n_components = 2, init = 'random', random_state = 10, perplexity = 100)\n",
    "\n",
    "# Use only 400 rows to shorten processing time\n",
    "tsne_df = tsne.fit_transform(df[:400])\n",
    "\n",
    "sns.set()\n",
    "# Initialize figure\n",
    "fig, ax = plt.subplots(figsize = (11.7, 8.27))\n",
    "sns.scatterplot(tsne_df[:, 0], tsne_df[:, 1], alpha = 0.5)\n",
    "\n",
    "# initialize list of texts\n",
    "texts = []\n",
    "words_to_plot = list(np.arange(0, 400, 10))\n",
    "\n",
    "# Append words to list\n",
    "for word in words_to_plot:\n",
    "    texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df.index[word], fontsize = 14))\n",
    "    \n",
    "# Plot text using adjust_text (because overlapping text is hard to read)\n",
    "adjust_text(texts, force_points = 0.4, force_text = 0.4, \n",
    "            expand_points = (2,1), expand_text = (1,2),\n",
    "            arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
