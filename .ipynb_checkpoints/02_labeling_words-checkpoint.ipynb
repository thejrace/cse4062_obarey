{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nPart 2: \\n    Objective: \\n    \\n    Step 1: Read labeled tokens and group them by defined categories.\\n\\n        Input:\\n            |     Token       |    Category   |\\n            |-----------------|---------------|\\n            |  B-person_name  |     Person    |\\n            |-----------------|---------------|\\n            | B-location_name |    Location   |\\n            |-----------------|---------------|\\n            |   B-film_actor  |    Person     |\\n\\n\\n    Output:\\n    \\n        Objective: This list will be used for calculating centroid for categories\\n        entity_categories = {\\n            'Person'   : { 'person_name', 'film_actor' },\\n            'Location' : { 'location_name' }\\n        }\\n        \\n        Objective: This list will be used for labeling words faster. \\n        Brief    : The reason for this list is that when we're labeling\\n        words, in order to find label we have to 'find' it in entity_categories. With this list in hand we can\\n        find the category without looping through entity_categories.\\n        entity_category_dictionary = {\\n            'person_name' : 'Person',\\n            'location_name' : 'Location',        \\n        }\\n        \\n        \\n    Step 2: Read dataset and label words with their corresponding categories.\\n        \\n        Output:\\n            labeled_words = {\\n                'Corina': 'Person',\\n                'Casanova': 'Person',\\n                'Şansölyesidir' : 'PersonType',\\n                ..\\n            }\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "\"\"\"\n",
    "Part 2: \n",
    "    Objective: \n",
    "    \n",
    "    Step 1: Read labeled tokens and group them by defined categories.\n",
    "\n",
    "        Input:\n",
    "            |     Token       |    Category   |\n",
    "            |-----------------|---------------|\n",
    "            |  B-person_name  |     Person    |\n",
    "            |-----------------|---------------|\n",
    "            | B-location_name |    Location   |\n",
    "            |-----------------|---------------|\n",
    "            |   B-film_actor  |    Person     |\n",
    "\n",
    "\n",
    "    Output:\n",
    "    \n",
    "        Objective: This list will be used for calculating centroid for categories\n",
    "        entity_categories = {\n",
    "            'Person'   : { 'person_name', 'film_actor' },\n",
    "            'Location' : { 'location_name' }\n",
    "        }\n",
    "        \n",
    "        Objective: This list will be used for labeling words faster. \n",
    "        Brief    : The reason for this list is that when we're labeling\n",
    "        words, in order to find label we have to 'find' it in entity_categories. With this list in hand we can\n",
    "        find the category without looping through entity_categories.\n",
    "        entity_category_dictionary = {\n",
    "            'person_name' : 'Person',\n",
    "            'location_name' : 'Location',        \n",
    "        }\n",
    "        \n",
    "        \n",
    "    Step 2: Read dataset and label words with their corresponding categories.\n",
    "        \n",
    "        Output:\n",
    "            labeled_words = {\n",
    "                'Corina': 'Person',\n",
    "                'Casanova': 'Person',\n",
    "                'Şansölyesidir' : 'PersonType',\n",
    "                ..\n",
    "            }\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#### Helpers ####\n",
    " \n",
    "# Removes token identifiers from string.\n",
    "def remove_token_identifiers(raw_token):\n",
    "    return raw_token.replace('B-', '').replace('I-', '')\n",
    "\n",
    "# Checks whether token is inner.\n",
    "def is_inner_token(raw_token):\n",
    "    return raw_token.find('I-') != -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to create entity categories: 0.0067 mins\n"
     ]
    }
   ],
   "source": [
    "#### STEP 1 ####\n",
    "\n",
    "t = time()\n",
    "my_printer = pprint.PrettyPrinter()\n",
    "\n",
    "# Read file\n",
    "df = pd.read_excel('data/labeled_categories_with_examples.xlsx')\n",
    "\n",
    "# Fallback category for the tokens does not belong to chosen categories\n",
    "fallback_category = 'Thing'\n",
    "# Main category list\n",
    "category_list = [\n",
    "    'Person', 'Location', 'Event', 'Organization', 'DateTime', 'PersonType', 'Currency', 'Nationality', 'Ethnicity', fallback_category\n",
    "]\n",
    "# Initialize category-entity dictionary\n",
    "entity_categories = {}\n",
    "entity_category_dictionary = {}\n",
    "\n",
    "for category in category_list:\n",
    "    entity_categories[category] = set({})\n",
    "\n",
    "# Read Excel file and fill the dictionary\n",
    "for row in df.index:\n",
    "    token = df['Token'][row]\n",
    "    category = df['Category'][row]\n",
    "    \n",
    "    # Fallback category check\n",
    "    if category == '' or category == 'Delete' or category == 'delete' or category == 'Thing' or category == 'thing':\n",
    "        entity_categories[fallback_category].add(token)\n",
    "        entity_category_dictionary[token] = fallback_category\n",
    "        continue\n",
    "    \n",
    "    # Undefined category check\n",
    "    if category not in category_list:\n",
    "        continue\n",
    "    \n",
    "    # Valid category\n",
    "    entity_categories[category].add(token)\n",
    "    entity_category_dictionary[token] = category\n",
    "                \n",
    "print('Time to create entity categories: {} mins'.format(round((time() - t) / 60, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person\n"
     ]
    }
   ],
   "source": [
    "#my_printer.pprint(entity_categories)\n",
    "print(entity_category_dictionary['person_name'])\n",
    "# my_printer.pprint(entity_categories['Location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Label           Word\n",
      "0      Person         Corina\n",
      "1      Person       Casanova\n",
      "2    Location        İsviçre\n",
      "3  PersonType  Şansölyesidir\n",
      "4  PersonType         avukat\n",
      "Time to label the words: 0.0073 mins\n"
     ]
    }
   ],
   "source": [
    " #### STEP 2 ####\n",
    "\n",
    "t = time()\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Read dataset\n",
    "with open(\"data/data_origin.DUMP\", encoding=\"utf8\") as tsv:\n",
    "    \n",
    "    labeled_words = []\n",
    "\n",
    "    counter = 0\n",
    "    for line in csv.reader(tsv, dialect=\"excel-tab\"):\n",
    "        tokenized = line[1]\n",
    "        sentence = line[2]\n",
    "        \n",
    "        all_words = nltk.word_tokenize(sentence)\n",
    "        all_tokens = nltk.word_tokenize(tokenized)\n",
    "        \n",
    "        sentence_valid_tokens = []\n",
    "        \n",
    "        #counter += 1\n",
    "        #if counter == 100:\n",
    "        #    break\n",
    "                \n",
    "        for x in range(len(all_words)):\n",
    "\n",
    "            try:\n",
    "                token_raw = all_tokens[x]\n",
    "                \n",
    "                if token_raw == 'O':\n",
    "                    continue\n",
    "                \n",
    "                token = remove_token_identifiers(token_raw)\n",
    "                    \n",
    "                if token not in entity_category_dictionary:\n",
    "                    continue\n",
    "                \n",
    "                label = entity_category_dictionary[token]\n",
    "                \n",
    "                word = all_words[x]\n",
    "                   \n",
    "                if word in labeled_words:\n",
    "                    continue\n",
    "                    \n",
    "                df = df.append({\n",
    "                    'Word': word,\n",
    "                    'Label': label,\n",
    "                }, ignore_index=True)\n",
    "                \n",
    "                labeled_words.append(word)\n",
    "                \n",
    "                #sys.stdout.write('\\r'+word + ' : ' + label)\n",
    "\n",
    "            except IndexError:\n",
    "                pass\n",
    "\n",
    "print(df.head())\n",
    "print('Time to label the words: {} mins'.format(round((time() - t) / 60, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Label           Word\n",
      "0       Person         Corina\n",
      "1       Person       Casanova\n",
      "2  Nationality        İsviçre\n",
      "3   PersonType  Şansölyesidir\n",
      "Time to save: 0.2443 mins\n"
     ]
    }
   ],
   "source": [
    "print(df.head(4))\n",
    "t = time()\n",
    "\n",
    "writer = pd.ExcelWriter('data/labeled_words_single_v2.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Convert the dataframe to an XlsxWriter Excel object.\n",
    "df.to_excel(writer, sheet_name='Sheet1', index=False)\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.save()\n",
    "print('Time to save: {} mins'.format(round((time() - t) / 60, 4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
