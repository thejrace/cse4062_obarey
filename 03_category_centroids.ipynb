{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nPart 2: \\n    Objective: \\n    \\n    Step 1: Read labeled tokens and group them by defined categories.\\n\\n        Input:\\n            |     Token       |    Category   |\\n            |-----------------|---------------|\\n            |  B-person_name  |     Person    |\\n            |-----------------|---------------|\\n            | B-location_name |    Location   |\\n            |-----------------|---------------|\\n            |   B-film_actor  |    Person     |\\n\\n\\n    Output:\\n    \\n        Objective: This list will be used for calculating centroid for categories\\n        entity_categories = {\\n            'Person'   : { 'person_name', 'film_actor' },\\n            'Location' : { 'location_name' }\\n        }\\n        \\n        Objective: This list will be used for labeling words faster. \\n        Brief    : The reason for this list is that when we're labeling\\n        words, in order to find label we have to 'find' it in entity_categories. With this list in hand we can\\n        find the category without looping through entity_categories.\\n        entity_category_dictionary = {\\n            'person_name' : 'Person',\\n            'location_name' : 'Location',        \\n        }\\n        \\n        \\n    Step 2: Read dataset and label words with their corresponding categories.\\n        \\n        Labeling action:\\n            \\n            Corina        |  B-politician_name     ----|  Person\\n\\t        Casanova      |  I-politician_name     ----|\\n\\t        ,             |  O\\n\\t        İsviçre       |  B-person_nationality  ----| Nationality\\n\\t        Federal       |  O\\n\\t        Şansölyesidir |  B-governmental_jurisdiction_basic_title  | PersonType\\n\\t        .             |  O\\n\\n        Output:\\n            labeled_words = {\\n                'Corina Casanova': 'Person',\\n                'İsviçre': 'Nationality',\\n                'Şansölyesidir' : 'PersonType',\\n                ..\\n            }\\n\\n\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import sys\n",
    "from time import time\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim.models.wrappers import FastText\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "category_list = [\n",
    "    'Person', 'Location', 'Event', 'Organization', 'DateTime', 'PersonType', 'Currency', 'Nationality', 'Ethnicity'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load pre trained model\n",
    "model = FastText.load_fasttext_format(model_file='models/fasttext/cc.tr.300.bin')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fetch sentences from our dataset\n",
    "\n",
    "t = time()\n",
    "\n",
    "sentences = []\n",
    "with open(\"data/data_origin.DUMP\", encoding=\"utf8\") as tsv:\n",
    "\n",
    "    for line in csv.reader(tsv, dialect=\"excel-tab\"):\n",
    "        sentence = line[2]\n",
    "\n",
    "        cleared_words = []\n",
    "\n",
    "        all_words = nltk.word_tokenize(sentence)\n",
    "\n",
    "        for word in all_words:\n",
    "            if word not in stopwords.words('turkish'):\n",
    "                cleared_words.append(word)\n",
    "\n",
    "        sentences.append(cleared_words)\n",
    "        \n",
    "print('Time to tokenize: {} mins'.format(round((time() - t) / 60, 2)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}